{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CIFAR Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the CIFAR10 dataset, go here: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "The training data is stored in 5 separate files, and we will alternate between them during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down\n",
      "[1 9 2 ..., 5 3 5]\n"
     ]
    }
   ],
   "source": [
    "#Load necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim\n",
    "import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    "currentCifar = 1\n",
    "cifartr = unpickle('train0321.txt')\n",
    "\n",
    "\n",
    "for i in range(len(cifartr)):\n",
    "    #print (np.average(bbb[i]))\n",
    "    #print (np.max(bbb[i]))\n",
    "    cifartr[i]=cifartr[i]-np.average(cifartr[i])\n",
    "    cifartr[i]=cifartr[i]/np.std(cifartr[i])\n",
    "vali=cifartr[73000:73200]\n",
    "cifartr=cifartr[0:73000]\n",
    "\n",
    "cifarTla = unpickle('newlabley2.txt')\n",
    "cifarTla=np.argmax(cifarTla,axis=1)\n",
    "\n",
    "valila=cifarTla[73000:73200]\n",
    "cifarTla=cifarTla[0:73000]\n",
    "print(\"down\")\n",
    "print(cifarTla)\n",
    "total_layers = 25 #Specify how deep we want our network\n",
    "units_between_stride = int(total_layers / 5)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet\n",
    "An implementation of a Residual Network as described in [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-24cff2f7a1e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m   \u001b[0minitial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0minput_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'input'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "def resUnit(input_layer,i):\n",
    "    with tf.variable_scope(\"res_unit\"+str(i)):\n",
    "        part1 = slim.batch_norm(input_layer,activation_fn=None)\n",
    "        part2 = tf.nn.relu(part1)\n",
    "        part3 = slim.conv2d(part2,64,[3,3],activation_fn=None)\n",
    "        part4 = slim.batch_norm(part3,activation_fn=None)\n",
    "        part5 = tf.nn.relu(part4)\n",
    "        part6 = slim.conv2d(part5,64,[3,3],activation_fn=None)\n",
    "        output = input_layer + part6\n",
    "        print(part6.shape)\n",
    "        return output\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_layer = tf.placeholder(shape=[None,32,32,1],dtype=tf.float32,name='input')\n",
    "label_layer = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "label_oh = slim.layers.one_hot_encoding(label_layer,10)\n",
    "\n",
    "layer1 = slim.conv2d(input_layer,64,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(0))\n",
    "for i in range(5):\n",
    "    for j in range(units_between_stride):\n",
    "        print(units_between_stride)\n",
    "        layer1 = resUnit(layer1,j + (i*units_between_stride))\n",
    "    layer1 = slim.conv2d(layer1,64,[3,3],stride=[2,2],normalizer_fn=slim.batch_norm,scope='conv_s_'+str(i))\n",
    "    \n",
    "top = slim.conv2d(layer1,10,[3,3],normalizer_fn=slim.batch_norm,activation_fn=None,scope='conv_top')\n",
    "print(\"top\",top.shape)\n",
    "output = slim.layers.softmax(slim.layers.flatten(top))\n",
    "\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(label_oh * tf.log(output) + 1e-10, axis=[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the network graph\n",
    "We can call the Tensorflow Board to provide a graphical representation of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "# def strip_consts(graph_def, max_const_size=32):\n",
    "#     \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "#     strip_def = tf.GraphDef()\n",
    "#     for n0 in graph_def.node:\n",
    "#         n = strip_def.node.add() \n",
    "#         n.MergeFrom(n0)\n",
    "#         if n.op == 'Const':\n",
    "#             tensor = n.attr['value'].tensor\n",
    "#             size = len(tensor.tensor_content)\n",
    "#             if size > max_const_size:\n",
    "#                 tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "#     return strip_def\n",
    "\n",
    "# def show_graph(graph_def, max_const_size=32):\n",
    "#     \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "#     if hasattr(graph_def, 'as_graph_def'):\n",
    "#         graph_def = graph_def.as_graph_def()\n",
    "#     strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "#     code = \"\"\"\n",
    "#         <script>\n",
    "#           function load() {{\n",
    "#             document.getElementById(\"{id}\").pbtxt = {data};\n",
    "#           }}\n",
    "#         </script>\n",
    "#         <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "#         <div style=\"height:600px\">\n",
    "#           <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "#         </div>\n",
    "#     \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "#     iframe = \"\"\"\n",
    "#         <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "#     \"\"\".format(code.replace('\"', '&quot;'))\n",
    "#     display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "#boundaries = [int(73000*2/batchsize*3+100000/batchsize*0),int(73000*2/batchsize*3+100000/batchsize*1),int(73000*2/batchsize*3+100000/batchsize*3),int(73000*2/batchsize*3+100000/batchsize*6)]\n",
    "boundaries = [int(73000/batch_size*3),int(73000/batch_size*6),int(73000/batch_size*6),int(73000/batch_size*10)]\n",
    "#values = [0.2e-4,0.18e-4,0.17e-4,0.16e-4,0.15e-4] #i changed it form 0.4e-4 to 1.5e-4\n",
    "values = [1.5e-4,1e-4,0.8e-4,0.6e-4,0.4e-4]\n",
    "lr = tf.train.piecewise_constant(global_step, boundaries, values)\n",
    "\n",
    "\n",
    "trainer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "update = trainer.minimize(loss,global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "currentCifar = 1\n",
    "total_steps = 73000/100*10\n",
    "l = []\n",
    "a = []\n",
    "aT = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver()\n",
    "    #saver.restore(sess, \"backup/resnigh1.ckpt\");print(\"load\")\n",
    "    i = 0\n",
    "    draw = range(10000)\n",
    "    while i < total_steps:\n",
    "        if i % (10000/batch_size) != 0:\n",
    "            batch_index = np.random.choice(draw,size=batch_size,replace=False)\n",
    "        else:\n",
    "            draw = range(10000)\n",
    "#             if currentCifar == 5:\n",
    "#                 currentCifar = 1\n",
    "#                 print (\"Switched CIFAR set to \" + str(currentCifar))\n",
    "#             else:\n",
    "#                 currentCifar = currentCifar + 1\n",
    "#                 print (\"Switched CIFAR set to \" + str(currentCifar))\n",
    "#             cifar = unpickle('./cifar10/data_batch_'+str(currentCifar))\n",
    "            batch_index = np.random.choice(draw,size=batch_size,replace=False)\n",
    "        x = cifartr[batch_index]\n",
    "        x = np.reshape(x,[batch_size,32,32,1],order='F')\n",
    "        y = np.reshape(np.array(cifarTla)[batch_index],[batch_size,1])\n",
    "        _,lossA,yP,LO = sess.run([update,loss,output,label_oh],feed_dict={input_layer:x,label_layer:np.hstack(y)})\n",
    "        accuracy = np.sum(np.equal(np.hstack(y),np.argmax(yP,1)))/float(len(y))\n",
    "        l.append(lossA)\n",
    "        a.append(accuracy)\n",
    "        if i % 10 == 0: \n",
    "            print (\"Step: \" + str(i) + \" Loss: \" + str(lossA) + \" Accuracy: \" + str(accuracy))\n",
    "            xT=np.reshape(vali,[-1,32,32,1],order='F')\n",
    "            yT = np.reshape(np.array(valila),[200])\n",
    "            lossT,yP = sess.run([loss,output],feed_dict={input_layer:xT,label_layer:yT})\n",
    "            valiaccuracy = np.sum(np.equal(yT,np.argmax(yP,1)))/float(len(yT))\n",
    "            \n",
    "            print(\"vali is \",valiaccuracy)\n",
    "            print(\"learning rate is\",lr)\n",
    "#         if i % 100 == 0: \n",
    "#             point = np.random.randint(0,10000-500)\n",
    "#             xT = cifarT['data'][point:point+500]\n",
    "#             xT = np.reshape(xT,[500,32,32,3],order='F')\n",
    "#             xT = (xT/256.0)\n",
    "#             xT = (xT - np.mean(xT,axis=0)) / np.std(xT,axis=0)\n",
    "#             yT = np.reshape(np.array(cifarT['labels'])[point:point+500],[500])\n",
    "#             lossT,yP = sess.run([loss,output],feed_dict={input_layer:xT,label_layer:yT})\n",
    "#             accuracy = np.sum(np.equal(yT,np.argmax(yP,1)))/float(len(yT))\n",
    "#             aT.append(accuracy)\n",
    "#             print( \"Test set accuracy: \" + str(accuracy))\n",
    "        i+= 1\n",
    "    saver.save(sess, \"backup/resnigh1.ckpt\")\n",
    "    \n",
    "#20 second to appear something FOR 5 LAYERES\n",
    "#5s second to appear the first number for 10 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lossT,yP = sess.run([loss,output],feed_dict={input_layer:xT,label_layer:yT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(l) #Plot training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(a) #Plot training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(aT) #Plot test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.max(aT) #Best test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = tf.train.AdamOptimizer(learning_rate=1.5e-4)\n",
    "# update = trainer.minimize(loss)\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "# batch_size = 100\n",
    "# currentCifar = 1\n",
    "# total_steps = 73000/100\n",
    "# l = []\n",
    "# a = []\n",
    "# aT = []\n",
    "# sess = tf.InteractiveSession()\n",
    "# sess.run(init)\n",
    "# saver = tf.train.Saver()\n",
    "#     #saver.restore(sess, \"codebackup/model4.ckpt\")\n",
    "# i = 0\n",
    "# draw = range(10000)\n",
    "# while i < total_steps:\n",
    "#     if i % (10000/batch_size) != 0:\n",
    "#         batch_index = np.random.choice(draw,size=batch_size,replace=False)\n",
    "#     else:\n",
    "#         draw = range(10000)\n",
    "# #             if currentCifar == 5:\n",
    "# #                 currentCifar = 1\n",
    "# #                 print (\"Switched CIFAR set to \" + str(currentCifar))\n",
    "# #             else:\n",
    "# #                 currentCifar = currentCifar + 1\n",
    "# #                 print (\"Switched CIFAR set to \" + str(currentCifar))\n",
    "# #             cifar = unpickle('./cifar10/data_batch_'+str(currentCifar))\n",
    "#         batch_index = np.random.choice(draw,size=batch_size,replace=False)\n",
    "#     x = cifartr[batch_index]\n",
    "#     x = np.reshape(x,[batch_size,32,32,1],order='F')\n",
    "# #         x = (x)\n",
    "# #         x = (x - np.mean(x,axis=0)) / np.std(x,axis=0)\n",
    "#     y = np.reshape(np.array(cifarTla)[batch_index],[batch_size,1])\n",
    "#     _,lossA,yP,LO = sess.run([update,loss,output,label_oh],feed_dict={input_layer:x,label_layer:np.hstack(y)})\n",
    "#     accuracy = np.sum(np.equal(np.hstack(y),np.argmax(yP,1)))/float(len(y))\n",
    "#     l.append(lossA)\n",
    "#     a.append(accuracy)\n",
    "#     if i % 10 == 0: print (\"Step: \" + str(i) + \" Loss: \" + str(lossA) + \" Accuracy: \" + str(accuracy))\n",
    "# #         if i % 100 == 0: \n",
    "# #             point = np.random.randint(0,10000-500)\n",
    "# #             xT = cifarT['data'][point:point+500]\n",
    "# #             xT = np.reshape(xT,[500,32,32,3],order='F')\n",
    "# #             xT = (xT/256.0)\n",
    "# #             xT = (xT - np.mean(xT,axis=0)) / np.std(xT,axis=0)\n",
    "# #             yT = np.reshape(np.array(cifarT['labels'])[point:point+500],[500])\n",
    "# #             lossT,yP = sess.run([loss,output],feed_dict={input_layer:xT,label_layer:yT})\n",
    "# #             accuracy = np.sum(np.equal(yT,np.argmax(yP,1)))/float(len(yT))\n",
    "# #             aT.append(accuracy)\n",
    "# #             print( \"Test set accuracy: \" + str(accuracy))\n",
    "#     i+= 1\n",
    "#10 second to appear something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
