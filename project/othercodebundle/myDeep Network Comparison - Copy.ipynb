{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CIFAR Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the CIFAR10 dataset, go here: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "The training data is stored in 5 separate files, and we will alternate between them during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down\n",
      "[1 9 2 ..., 5 3 5]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim\n",
    "import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    "currentCifar = 1\n",
    "cifartr = unpickle('train0321.txt')\n",
    "\n",
    "\n",
    "for i in range(len(cifartr)):\n",
    "    #print (np.average(bbb[i]))\n",
    "    #print (np.max(bbb[i]))\n",
    "    cifartr[i]=cifartr[i]-np.average(cifartr[i])\n",
    "    cifartr[i]=cifartr[i]/np.std(cifartr[i])\n",
    "vali=cifartr[73000:73200]\n",
    "cifartr=cifartr[0:73000]\n",
    "\n",
    "cifarTla = unpickle('newlabley2.txt')\n",
    "cifarTla=np.argmax(cifarTla,axis=1)\n",
    "\n",
    "valila=cifarTla[73000:73200]\n",
    "cifarTla=cifarTla[0:73000]\n",
    "print(\"down\")\n",
    "print(cifarTla)\n",
    "total_layers = 25 #Specify how deep we want our network\n",
    "units_between_stride = 5#int(total_layers / 5)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet\n",
    "\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)An implementation of a Residual Network as described in [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"OneHotEncoding/one_hot:0\", shape=(?, 10), dtype=float32)\n",
      "3\n",
      "(?, 32, 32, 32)\n",
      "3\n",
      "(?, 32, 32, 32)\n",
      "3\n",
      "(?, 32, 32, 32)\n",
      "4\n",
      "(?, 16, 16, 64)\n",
      "4\n",
      "(?, 16, 16, 64)\n",
      "4\n",
      "(?, 16, 16, 64)\n",
      "4\n",
      "(?, 16, 16, 64)\n",
      "6\n",
      "(?, 8, 8, 128)\n",
      "6\n",
      "(?, 8, 8, 128)\n",
      "6\n",
      "(?, 8, 8, 128)\n",
      "6\n",
      "(?, 8, 8, 128)\n",
      "6\n",
      "(?, 8, 8, 128)\n",
      "6\n",
      "(?, 8, 8, 128)\n",
      "3\n",
      "(?, 4, 4, 256)\n",
      "3\n",
      "(?, 4, 4, 256)\n",
      "3\n",
      "(?, 4, 4, 256)\n",
      "top (?, 1, 1, 10)\n",
      "output (?, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "def resUnit0(input_layer,i):\n",
    "    with tf.variable_scope(\"res_unit\"+str(i)):\n",
    "        part1 = slim.batch_norm(input_layer,activation_fn=None)\n",
    "        part2 = tf.nn.relu(part1)\n",
    "        part3 = slim.conv2d(part2,32,[3,3],activation_fn=None)\n",
    "        part4 = slim.batch_norm(part3,activation_fn=None)\n",
    "        part5 = tf.nn.relu(part4)\n",
    "        part6 = slim.conv2d(part5,32,[3,3],activation_fn=None)\n",
    "        print(part6.shape)\n",
    "        output = input_layer + part6\n",
    "        return output\n",
    "def resUnit1(input_layer,i):\n",
    "    with tf.variable_scope(\"res_unit\"+str(i)):\n",
    "        part1 = slim.batch_norm(input_layer,activation_fn=None)\n",
    "        part2 = tf.nn.relu(part1)\n",
    "        part3 = slim.conv2d(part2,64,[3,3],activation_fn=None)\n",
    "        part4 = slim.batch_norm(part3,activation_fn=None)\n",
    "        part5 = tf.nn.relu(part4)\n",
    "        part6 = slim.conv2d(part5,64,[3,3],activation_fn=None)\n",
    "        print(part6.shape)\n",
    "        output = input_layer + part6\n",
    "        return output\n",
    "def resUnit2(input_layer,i):\n",
    "    with tf.variable_scope(\"res_unit\"+str(i)):\n",
    "        part1 = slim.batch_norm(input_layer,activation_fn=None)\n",
    "        part2 = tf.nn.relu(part1)\n",
    "        part3 = slim.conv2d(part2,128,[3,3],activation_fn=None)\n",
    "        part4 = slim.batch_norm(part3,activation_fn=None)\n",
    "        part5 = tf.nn.relu(part4)\n",
    "        part6 = slim.conv2d(part5,128,[3,3],activation_fn=None)\n",
    "        print(part6.shape)\n",
    "        output = input_layer + part6\n",
    "        return output\n",
    "def resUnit3(input_layer,i):\n",
    "    with tf.variable_scope(\"res_unit\"+str(i)):\n",
    "        part1 = slim.batch_norm(input_layer,activation_fn=None)\n",
    "        part2 = tf.nn.relu(part1)\n",
    "        part3 = slim.conv2d(part2,256,[3,3],activation_fn=None)\n",
    "        part4 = slim.batch_norm(part3,activation_fn=None)\n",
    "        part5 = tf.nn.relu(part4)\n",
    "        part6 = slim.conv2d(part5,256,[3,3],activation_fn=None)\n",
    "        print(part6.shape)\n",
    "        output = input_layer + part6\n",
    "        return output\n",
    "def resUnit4(input_layer,i):\n",
    "    with tf.variable_scope(\"res_unit\"+str(i)):\n",
    "        part1 = slim.batch_norm(input_layer,activation_fn=None)\n",
    "        part2 = tf.nn.relu(part1)\n",
    "        part3 = slim.conv2d(part2,256,[3,3],activation_fn=None)\n",
    "        part4 = slim.batch_norm(part3,activation_fn=None)\n",
    "        part5 = tf.nn.relu(part4)\n",
    "        part6 = slim.conv2d(part5,256,[3,3],activation_fn=None)\n",
    "        output = input_layer + part6\n",
    "        print(part6.shape)\n",
    "        return output\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_layer = tf.placeholder(shape=[None,32,32,1],dtype=tf.float32,name='input')\n",
    "label_layer = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "label_oh = slim.layers.one_hot_encoding(label_layer,10)\n",
    "print(label_oh)\n",
    "\n",
    "layer1 = slim.conv2d(input_layer,32,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(0))\n",
    "units_between_stride = 3\n",
    "for j in range(units_between_stride):\n",
    "    print(units_between_stride)\n",
    "    layer1 = resUnit0(layer1,j + 0)\n",
    "layer1 = slim.conv2d(layer1,64,[3,3],stride=[2,2],normalizer_fn=slim.batch_norm,scope='conv_s_'+str(0))\n",
    "units_between_stride = 4\n",
    "for j in range(units_between_stride):\n",
    "    print(units_between_stride)\n",
    "    layer1 = resUnit1(layer1,j + (1*units_between_stride))\n",
    "layer1 = slim.conv2d(layer1,128,[3,3],stride=[2,2],normalizer_fn=slim.batch_norm,scope='conv_s_'+str(1))\n",
    "\n",
    "\n",
    "units_between_stride = 6\n",
    "for j in range(units_between_stride):\n",
    "    print(units_between_stride)\n",
    "    layer1 = resUnit2(layer1,j + (2*units_between_stride))\n",
    "aaaa=2\n",
    "layer1 = slim.conv2d(layer1,256,[3,3],stride=[aaaa,aaaa],normalizer_fn=slim.batch_norm,scope='conv_s_'+str(2))\n",
    "units_between_stride = 3\n",
    "for j in range(units_between_stride):\n",
    "    print(units_between_stride)\n",
    "    layer1 = resUnit3(layer1,j + (3*units_between_stride))\n",
    "layer1 = slim.conv2d(layer1,256,[3,3],stride=[aaaa,aaaa],normalizer_fn=slim.batch_norm,scope='conv_s_'+str(3))\n",
    "# for j in range(units_between_stride):\n",
    "#     print(units_between_stride)\n",
    "#     layer1 = resUnit4(layer1,j + (4*units_between_stride))\n",
    "# layer1 = slim.conv2d(layer1,256,[3,3],stride=[2,2],normalizer_fn=slim.batch_norm,scope='conv_s_'+str(4))\n",
    "# print(\"layer1\",layer1.shape)\n",
    "# W_fc1 = weight_variable([64 * 256, 2048])\n",
    "# b_fc1 = bias_variable([2048])\n",
    "# h_pool2_flat = tf.reshape(layer1, [-1, 64*256])\n",
    "# h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "\n",
    "\n",
    "# W_fc2 = weight_variable([2048, 10])\n",
    "# b_fc2 = bias_variable([10])\n",
    "# y_conv = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "layer1=tf.nn.avg_pool(layer1, [1,2,2,1], [1,2,2,1], padding=\"VALID\", name=None)\n",
    "\n",
    "\n",
    "    \n",
    "top = slim.conv2d(layer1,10,[3,3],normalizer_fn=slim.batch_norm,activation_fn=None,scope='conv_top')\n",
    "print(\"top\",top.shape)\n",
    "output = slim.layers.softmax(slim.layers.flatten(top))\n",
    "print(\"output\",output.shape) #must be 10\n",
    "#loss = tf.reduce_mean(-tf.reduce_sum(label_oh * tf.log(output) + 1e-10, axis=[1]))\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=label_oh, logits=output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the network graph\n",
    "We can call the Tensorflow Board to provide a graphical representation of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Step: 0 Loss: 2.30998 Accuracy: 0.04\n",
      "vali is  0.145\n",
      "vali is  0.735\n",
      "1\n",
      "Step: 0 Loss: 1.86113 Accuracy: 0.9\n",
      "vali is  0.735\n",
      "vali is  0.805\n",
      "2\n",
      "Step: 0 Loss: 1.79417 Accuracy: 0.91\n",
      "vali is  0.805\n",
      "vali is  0.865\n",
      "3\n",
      "Step: 0 Loss: 1.76114 Accuracy: 0.92\n",
      "vali is  0.865\n",
      "vali is  0.88\n",
      "4\n",
      "Step: 0 Loss: 1.72806 Accuracy: 0.97\n",
      "vali is  0.88\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4cd6a7d659f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'F'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlossA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myP\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLO\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel_oh\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel_layer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myP\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlossA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "#boundaries = [int(73000*2/batchsize*3+100000/batchsize*0),int(73000*2/batchsize*3+100000/batchsize*1),int(73000*2/batchsize*3+100000/batchsize*3),int(73000*2/batchsize*3+100000/batchsize*6)]\n",
    "boundaries = [int(73000/batch_size*3),int(73000/batch_size*6),int(73000/batch_size*6),int(73000/batch_size*10)]\n",
    "#values = [0.2e-4,0.18e-4,0.17e-4,0.16e-4,0.15e-4] #i changed it form 0.4e-4 to 1.5e-4\n",
    "values = [1.5e-4,1e-4,0.8e-4,0.6e-4,0.4e-4]\n",
    "lr = tf.train.piecewise_constant(global_step, boundaries, values)\n",
    "\n",
    "\n",
    "trainer = tf.train.AdamOptimizer(learning_rate=0.2e-4)\n",
    "update = trainer.minimize(loss,global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "currentCifar = 1\n",
    "epoch=0;\n",
    "total_steps = 73000/100\n",
    "l = []\n",
    "a = []\n",
    "aT = []\n",
    "\n",
    "hh=3;\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "#with tf.Session() as sess:\n",
    "sess.run(init)\n",
    "saver = tf.train.Saver()\n",
    "#saver.restore(sess, \"backup/resnigh3.ckpt\");print(\"load\")\n",
    "\n",
    "\n",
    "for times in range(epoch): \n",
    "    i = 0\n",
    "    for i in range(int(total_steps)) :\n",
    "        x = cifartr[i*batch_size:i*batch_size+batch_size]\n",
    "        x = np.reshape(x,[batch_size,32,32,1],order='F')\n",
    "        y = np.reshape(np.array(cifarTla)[i*batch_size:i*batch_size+batch_size],[batch_size,1])\n",
    "        _,lossA,yP,LO = sess.run([update,loss,output,label_oh],feed_dict={input_layer:x,label_layer:np.hstack(y)})\n",
    "        accuracy = np.sum(np.equal(np.hstack(y),np.argmax(yP,1)))/float(len(y))\n",
    "        l.append(lossA)\n",
    "        a.append(accuracy)\n",
    "        if i % 300 == 0: \n",
    "            print (\"Step: \" + str(i) + \" Loss: \" + str(lossA) + \" Accuracy: \" + str(accuracy))\n",
    "            xT=np.reshape(vali,[-1,32,32,1],order='F')\n",
    "            yT = np.reshape(np.array(valila),[200])\n",
    "            lossT,yP = sess.run([loss,output],feed_dict={input_layer:xT,label_layer:yT})\n",
    "            valiaccuracy = np.sum(np.equal(yT,np.argmax(yP,1)))/float(len(yT))\n",
    "            \n",
    "            print(\"vali is \",valiaccuracy)\n",
    "\n",
    "        \n",
    "        \n",
    "epochofextra=1\n",
    "\n",
    "for times in range(epochofextra): \n",
    "    for k in range(10):\n",
    "        ss=str(k)\n",
    "        print(k)\n",
    "        fname=\"extra/e\"+ss+\".txt\" \n",
    "        exdata = unpickle(fname)\n",
    "        fname = \"extra/l\" + ss + \".txt\"\n",
    "        la = unpickle(fname)\n",
    "        la=np.argmax(la,axis=1)\n",
    "\n",
    "        for i in range(len(exdata)):\n",
    "                # print (np.average(bbb[i]))\n",
    "                # print (np.max(bbb[i]))\n",
    "            exdata[i] = exdata[i] - np.average(exdata[i])\n",
    "            exdata[i] = exdata[i] / np.std(exdata[i])\n",
    "    \n",
    "    \n",
    "    \n",
    "       \n",
    "        for i in range(int(50000/batch_size)):\n",
    "\n",
    "            x = exdata[i*batch_size:i*batch_size+batch_size]\n",
    "            x = np.reshape(x,[batch_size,32,32,1],order='F')\n",
    "            y = np.reshape(np.array(la)[i*batch_size:i*batch_size+batch_size],[batch_size,1])\n",
    "            _,lossA,yP,LO = sess.run([update,loss,output,label_oh],feed_dict={input_layer:x,label_layer:np.hstack(y)})\n",
    "            accuracy = np.sum(np.equal(np.hstack(y),np.argmax(yP,1)))/float(len(y))\n",
    "            l.append(lossA)\n",
    "            a.append(accuracy)\n",
    "            if i  == 0: \n",
    "                print (\"Step: \" + str(i) + \" Loss: \" + str(lossA) + \" Accuracy: \" + str(accuracy))\n",
    "                xT=np.reshape(vali,[-1,32,32,1],order='F')\n",
    "                yT = np.reshape(np.array(valila),[200])\n",
    "                lossT,yP = sess.run([loss,output],feed_dict={input_layer:xT,label_layer:yT})\n",
    "                valiaccuracy = np.sum(np.equal(yT,np.argmax(yP,1)))/float(len(yT))\n",
    "            \n",
    "                print(\"vali is \",valiaccuracy)\n",
    "        for i in range(int(73000/ batch_size)):\n",
    "            x = cifartr[i*batch_size:i*batch_size+batch_size]\n",
    "            x = np.reshape(x,[batch_size,32,32,1],order='F')\n",
    "            y = np.reshape(np.array(cifarTla)[i*batch_size:i*batch_size+batch_size],[batch_size,1])\n",
    "            _,lossA,yP,LO = sess.run([update,loss,output,label_oh],feed_dict={input_layer:x,label_layer:np.hstack(y)})\n",
    "            accuracy = np.sum(np.equal(np.hstack(y),np.argmax(yP,1)))/float(len(y))\n",
    "            l.append(lossA)\n",
    "            a.append(accuracy)\n",
    "\n",
    "        xT=np.reshape(vali,[-1,32,32,1],order='F')\n",
    "        yT = np.reshape(np.array(valila),[200])\n",
    "        lossT,yP = sess.run([loss,output],feed_dict={input_layer:xT,label_layer:yT})\n",
    "        valiaccuracy = np.sum(np.equal(yT,np.argmax(yP,1)))/float(len(yT))           \n",
    "        print(\"vali is \",valiaccuracy)      \n",
    "saver.save(sess, \"backup/resnigh3.ckpt\")\n",
    "print(\"saved\")   \n",
    "#20 second to appear something FOR 5 LAYERES\n",
    "#5s second to appear the first number for 10 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(l) #Plot training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(a) #Plot training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(aT) #Plot test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.max(aT) #Best test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "saver.save(sess, \"backup/resnigh3.ckpt\")\n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vali is  0.915\n"
     ]
    }
   ],
   "source": [
    "xT=np.reshape(vali,[-1,32,32,1],order='F')\n",
    "yT = np.reshape(np.array(valila),[200])\n",
    "lossT,yP = sess.run([loss,output],feed_dict={input_layer:xT,label_layer:yT})\n",
    "valiaccuracy = np.sum(np.equal(yT,np.argmax(yP,1)))/float(len(yT))\n",
    "            \n",
    "print(\"vali is \",valiaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
